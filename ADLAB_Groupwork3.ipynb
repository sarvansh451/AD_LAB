{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e34160-a3ad-469c-8c64-5c45fc85cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5dc5da-c117-4b32-8476-001778c71386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 21us/step\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten images\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a461ff-2ba5-4eef-8a0a-5ba07d2c96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de70145-37b3-4149-8e9a-e2919eaead10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA (keeping 95% variance)\n",
    "pca = PCA(0.95)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b713e1-0599-4ee5-9bb6-25adec883113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1cd4ff4-9823-4d75-92a7-4c0102e70ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, x_train, x_test, y_train, y_test):\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        results.append([name, accuracy, precision, recall, f1])\n",
    "    return pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5bc0bc-9b59-42e1-b2ec-6941b044aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models before and after PCA\n",
    "table_before_pca = evaluate_models(models, x_train, x_test, y_train, y_test)\n",
    "table_after_pca = evaluate_models(models, x_train_pca, x_test_pca, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9130c27a-cf2b-44c4-9e7a-b59e70ff7d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Before PCA:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression    0.9216   0.920718  0.920517  0.920548\n",
      "1                  SVM    0.9660   0.966080  0.965592  0.965758\n",
      "2        Random Forest    0.9696   0.969439  0.969345  0.969369\n",
      "3          Naive Bayes    0.5240   0.669931  0.517009  0.469186\n",
      "4                  KNN    0.9443   0.944211  0.943586  0.943686\n",
      "5        Decision Tree    0.8765   0.874853  0.874880  0.874772\n",
      "\n",
      "Performance After PCA:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression    0.9264   0.925624  0.925351  0.925391\n",
      "1                  SVM    0.9686   0.968623  0.968256  0.968372\n",
      "2        Random Forest    0.9380   0.937706  0.937284  0.937371\n",
      "3          Naive Bayes    0.4546   0.556085  0.447653  0.406084\n",
      "4                  KNN    0.9486   0.948520  0.947944  0.948043\n",
      "5        Decision Tree    0.8243   0.822299  0.822089  0.822039\n"
     ]
    }
   ],
   "source": [
    "# Print performance tables\n",
    "print(\"Performance Before PCA:\")\n",
    "print(table_before_pca)\n",
    "print(\"\\nPerformance After PCA:\")\n",
    "print(table_after_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b572ca-4038-448e-b3d7-0acf4ddbf041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert MNIST data to CNN format\n",
    "x_train = np.stack((x_train,)*3, axis=-1)\n",
    "x_test = np.stack((x_test,)*3, axis=-1)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44d1a664-86dd-4877-9dd6-12082b878157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VGG16 Model\n",
    "def create_vgg16():\n",
    "    base_model = tf.keras.applications.VGG16(weights=None, input_shape=(28, 28, 3), classes=10)\n",
    "    model = tf.keras.models.Sequential([base_model, tf.keras.layers.Flatten(), tf.keras.layers.Dense(10, activation='softmax')])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define ResNet Model\n",
    "def create_resnet():\n",
    "    base_model = tf.keras.applications.ResNet50(weights=None, input_shape=(28, 28, 3), classes=10)\n",
    "    model = tf.keras.models.Sequential([base_model, tf.keras.layers.Flatten(), tf.keras.layers.Dense(10, activation='softmax')])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef95dd-4bfa-49cc-a045-d5311d69e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 2s/step - accuracy: 0.0960 - loss: 2.3222 - val_accuracy: 0.0982 - val_loss: 2.3062\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 2s/step - accuracy: 0.0992 - loss: 2.3045 - val_accuracy: 0.1135 - val_loss: 2.3020\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 2s/step - accuracy: 0.1165 - loss: 2.3002 - val_accuracy: 0.1135 - val_loss: 2.3015\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 2s/step - accuracy: 0.1103 - loss: 2.3009 - val_accuracy: 0.1135 - val_loss: 2.3014\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 2s/step - accuracy: 0.1177 - loss: 2.3010 - val_accuracy: 0.1135 - val_loss: 2.3015\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2600s\u001b[0m 8s/step - accuracy: 0.1067 - loss: 2.3018 - val_accuracy: 0.1135 - val_loss: 2.3014\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 906ms/step - accuracy: 0.1085 - loss: 2.3012 - val_accuracy: 0.1135 - val_loss: 2.3014\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 1s/step - accuracy: 0.1129 - loss: 2.3002 - val_accuracy: 0.1135 - val_loss: 2.3014\n",
      "Epoch 9/10\n",
      "\u001b[1m 63/313\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:06:53\u001b[0m 74s/step - accuracy: 0.1078 - loss: 2.2994"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reduce dataset size to 1000 samples\n",
    "x_train, y_train = x_train[:10000], y_train[:10000]\n",
    "x_test, y_test = x_test[:10000], y_test[:10000]\n",
    "\n",
    "# Normalize data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert MNIST data to 3-channel format\n",
    "x_train = np.stack((x_train,)*3, axis=-1)\n",
    "x_test = np.stack((x_test,)*3, axis=-1)\n",
    "\n",
    "# Resize images to 32x32 for VGG16 and ResNet\n",
    "x_train = tf.image.resize(x_train, (32, 32)).numpy()\n",
    "x_test = tf.image.resize(x_test, (32, 32)).numpy()\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define VGG16 Model\n",
    "def create_vgg16():\n",
    "    base_model = tf.keras.applications.VGG16(weights=None, input_shape=(32, 32, 3), classes=10)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define ResNet Model\n",
    "def create_resnet():\n",
    "    base_model = tf.keras.applications.ResNet50(weights=None, input_shape=(32, 32, 3), classes=10)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate VGG16\n",
    "vgg16_model = create_vgg16()\n",
    "vgg16_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "vgg16_score = vgg16_model.evaluate(x_test, y_test)\n",
    "print(\"VGG16 Accuracy:\", vgg16_score[1])\n",
    "\n",
    "# Train and evaluate ResNet\n",
    "resnet_model = create_resnet()\n",
    "resnet_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "resnet_score = resnet_model.evaluate(x_test, y_test)\n",
    "print(\"ResNet Accuracy:\", resnet_score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb8b8c-2fc8-4067-9cca-94c5b8c3aa82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb9e24-1b14-4c7f-9128-c888d42fcfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5b3f0-86cc-4334-9d79-6b502b14874d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
